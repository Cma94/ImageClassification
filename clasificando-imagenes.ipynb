{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'2.2.4'"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import keras\n",
    "from PIL import ImageFile\n",
    "ImageFile.LOAD_TRUNCATED_IMAGES = True\n",
    "keras.__version__"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Utilizando convnets con datasets pequeños\n",
    "\n",
    "Lo primero que tienes que hacer es descargarte el dataset de https://lara.web.cern.ch/lara/train.zip en la terminal de Jupyter y descomprimirlo en la misma carpeta donde se encuentra esta libreta. \n",
    "\n",
    "Para descargar otro conjunto de datos desde imagenet se puede descargar la lista con las URL a las imágenes y usar `wget -i`\n",
    "\n",
    "\n",
    "\n",
    "## Entrenando desde 0 una convNet\n",
    "\n",
    "Entrenar un modelo de clasificación de imágenes con muy pocos datos es una situación común en la que te encontrarás si acabas dedicándote a hacer Computer Vision en un contexto profesional. \n",
    "\n",
    "Tener \"pocas\" muestras puede significar cualquier cosa entre unos pocos cientos y unas pocas decenas de miles de imágenes. Vamos a ilustrar aqui un ejemplo práctico: vamos a centrarnos en clasificar imágenes como \"perros\" y \"gatos\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## La relevancia del Deep Learning en problemas con pocos datos\n",
    "\n",
    "Quizás habrás oido muchas veces que el Deep Learning solo funciona cuando se tienen grandes cantidades de datos. Esto en parte es verdad: una de las características del Deep learning es que puede encontrar características interesantes a partir del dataset de entrenamiento por si mismo, y esto a priori es más sencillo cuando se tienen muchos ejemplos disponibles, especialmente en el caso de tener datasets de input con una alta dimensionalidad, como es el caso de las imágenes.\n",
    "\n",
    "Sin embargo, lo que constituye un dataset \"grande\" es relativo. Concretamente relativo al tamaño y la profundidad de la red que estamos intentando entrenar. No es posible enrenar una convnet para que resulta un problema completo con solo unas decenas de ejemplos, pero unos pocos cientos puede ser suficiente si el modelo está bien montado (entenderemos que significa bien \"montado\" a lo largo del curso de Deep Learning).\n",
    "\n",
    "Como las convnets aprenden características locales, invariantes bajo translaciones, son muy eficientes en cuanto al número de imágenes necesarias para llevar a cabo problemas perceptuales. Así que entrenar una convnet desde 0 con un dataset no muy grande aún nos puede llevar a resultados razonables como veremos aqui.\n",
    "\n",
    "Pero hay más aún: los modelos de Deep Learning son altamente \"reciclables\". Uno puede coger, por ejemplo, un problema de clasificación de imagen y un convertidor de voz a texto entrenado sobre un dataset muy grande y luego reutilizarlo para resolver otro problema completamente distinto solo añadiéndole pequeñas modificaciones. Más especificamente, en el caso de Computer Vision, muchos modelos pre-entrenados (normalmente entrenados en el dataset ImageNet) son hechos publicos para que uno pueda descargarlos y utilizarlos para crear potentes modelos de Computer Vision con muy pocos datos. \n",
    "\n",
    "Pero aqui nos vamos a limitar a correr un ejemplo sencillito. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Los datos\n",
    "\n",
    "El dataset de gatos vs perros que utilizamos no es un paquete de Keras. Se publicó en Kaggle.com como parte de un problema de Computer Vision a finales de 2013, cuando todavía las ConvNets no eran tan populares. \n",
    "\n",
    "Las imágenes son JPGEs de resolución media. Tiene este aspecto:\n",
    "\n",
    "![cats_vs_dogs_samples](https://s3.amazonaws.com/book.keras.io/img/ch5/cats_vs_dogs_samples.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "No es ninguna sorpresa que la competición de gatos vs perros de Kaggle en 2013 fuera ganada por ConvNets. Los mejores pudieron alcanzar una precisión de hasta 95%. En nuestro ejemplo vamos a quedarnos todavía lejos de esta precisión, pero durante el curso de Deep Learning hemos aprendido como acercarnos a este valor utilizando diversos métodos para mejorar el rendimiento de las redes neuronales. Hay que tener en cuenta que en este ejemplo estamos entrenando aproximadamente sobre solo el 10% de los datos que se utilizaron para el concurso. \n",
    "Después de descargar el dataset y descomprimirlo, vamos a crear un nuevo dataset que contiene tres subsets: un set de training que contiene 1000 imágenes de cada clase, un set de validación con 500 imágenes de cada clase, y finalmente un set de test con 500 imágenes de cada clase.\n",
    "\n",
    "Aqui tenemos unas cuantas líneas de código que nos hacen este reparto automáticamente:\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os, shutil"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carlos\n"
     ]
    }
   ],
   "source": [
    "cd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "ename": "FileNotFoundError",
     "evalue": "[WinError 3] El sistema no puede encontrar la ruta especificada: '/home/jovyan/work/MachineLearning1/gusanos_and_insectos_small'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mFileNotFoundError\u001b[0m                         Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-3-3b89ee5d7c96>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     11\u001b[0m \u001b[1;31m# store our smaller dataset\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     12\u001b[0m \u001b[0mbase_dir\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m'/home/jovyan/work/MachineLearning1/gusanos_and_insectos_small'\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 13\u001b[1;33m \u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmkdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mbase_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     14\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     15\u001b[0m \u001b[1;31m# Directories for our training,\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mFileNotFoundError\u001b[0m: [WinError 3] El sistema no puede encontrar la ruta especificada: '/home/jovyan/work/MachineLearning1/gusanos_and_insectos_small'"
     ]
    }
   ],
   "source": [
    "#wget -i http://image-net.org/api/text/imagenet.synset.geturls?wnid=n02311617 -t 1 -T 3\n",
    "#wget -i http://image-net.org/api/text/imagenet.synset.geturls?wnid=n02309337 -t 1 -T 3\n",
    "\n",
    "\n",
    "\n",
    "# The path to the directory where the original\n",
    "# dataset was uncompressed\n",
    "original_dataset_dir = '/home/jovyan/work/MachineLearning1/train'\n",
    "\n",
    "# The directory where we will\n",
    "# store our smaller dataset\n",
    "base_dir = '/home/jovyan/work/MachineLearning1/gusanos_and_insectos_small'\n",
    "os.mkdir(base_dir)\n",
    "\n",
    "# Directories for our training,\n",
    "# validation and test splits\n",
    "train_dir = os.path.join(base_dir, 'train')\n",
    "os.mkdir(train_dir)\n",
    "validation_dir = os.path.join(base_dir, 'validation')\n",
    "os.mkdir(validation_dir)\n",
    "test_dir = os.path.join(base_dir, 'test')\n",
    "os.mkdir(test_dir)\n",
    "\n",
    "# Directory with our training cat pictures\n",
    "train_gusanos_dir = os.path.join(train_dir, 'gusanos')\n",
    "os.mkdir(train_gusanos_dir)\n",
    "\n",
    "# Directory with our training dog pictures\n",
    "train_insectos_dir = os.path.join(train_dir, 'insectos')\n",
    "os.mkdir(train_insectos_dir)\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "validation_gusanos_dir = os.path.join(validation_dir, 'gusanos')\n",
    "os.mkdir(validation_gusanos_dir)\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "validation_insectos_dir = os.path.join(validation_dir, 'insectos')\n",
    "os.mkdir(validation_insectos_dir)\n",
    "\n",
    "# Directory with our validation cat pictures\n",
    "test_gusanos_dir = os.path.join(test_dir, 'gusanos')\n",
    "os.mkdir(test_gusanos_dir)\n",
    "\n",
    "# Directory with our validation dog pictures\n",
    "test_insectos_dir = os.path.join(test_dir, 'insectos')\n",
    "os.mkdir(test_insectos_dir)\n",
    "\n",
    "# Copy first 1000 cat images to train_cats_dir\n",
    "fnames = ['Gusano_{:04d}.jpg'.format(i) for i in range(1,700)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_gusanos_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "\n",
    "# Copy next 500 cat images to validation_cats_dir\n",
    "fnames = ['Gusano_{:04d}.jpg'.format(i) for i in range(700, 816)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_gusanos_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 cat images to test_cats_dir\n",
    "fnames = ['Gusano_{:04d}.jpg'.format(i) for i in range(816, 932)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_gusanos_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy first 1000 dog images to train_dogs_dir\n",
    "fnames = ['Insecto_{:04d}.jpg'.format(i) for i in range(1,700)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(train_insectos_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to validation_dogs_dir\n",
    "fnames = ['Insecto_{:04d}.jpg'.format(i) for i in range(700, 816)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(validation_insectos_dir, fname)\n",
    "    shutil.copyfile(src, dst)\n",
    "    \n",
    "# Copy next 500 dog images to test_dogs_dir\n",
    "fnames = ['Insecto_{:04d}.jpg'.format(i) for i in range(816, 932)]\n",
    "for fname in fnames:\n",
    "    src = os.path.join(original_dataset_dir, fname)\n",
    "    dst = os.path.join(test_insectos_dir, fname)\n",
    "    shutil.copyfile(src, dst)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As a sanity check, let's count how many pictures we have in each training split (train/validation/test):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_gusanos_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-4-730e41c4b019>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'total training cat images:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_gusanos_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_gusanos_dir' is not defined"
     ]
    }
   ],
   "source": [
    "print('total training cat images:', len(os.listdir(train_gusanos_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'train_insectos_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-5-695a034390a8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'total training dog images:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtrain_insectos_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'train_insectos_dir' is not defined"
     ]
    }
   ],
   "source": [
    "print('total training dog images:', len(os.listdir(train_insectos_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'validation_gusanos_dir' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-6-88e4f6a8640f>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[1;32m----> 1\u001b[1;33m \u001b[0mprint\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;34m'total validation cat images:'\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mos\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mlistdir\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mvalidation_gusanos_dir\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m: name 'validation_gusanos_dir' is not defined"
     ]
    }
   ],
   "source": [
    "print('total validation cat images:', len(os.listdir(validation_gusanos_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total validation dog images: 116\n"
     ]
    }
   ],
   "source": [
    "print('total validation dog images:', len(os.listdir(validation_insectos_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test cat images: 116\n"
     ]
    }
   ],
   "source": [
    "print('total test cat images:', len(os.listdir(test_gusanos_dir)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "total test dog images: 116\n"
     ]
    }
   ],
   "source": [
    "print('total test dog images:', len(os.listdir(test_insectos_dir)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\"\n",
    "Así que efectivamente tenemos 2000 imágenes de entrenamiento, 1000 imágenes de validación y 1000 imágenes de test. En cada uno de estos subsets hay el mismo número de ejemplos de cada clase: esto es lo que se llama un sistema de clasificación binario balanceado, lo cual significa que nuestra precisión de clasificación será una métrica adecuada del éxito de nuestra solución."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Construyendo nuestra red\n",
    "\n",
    "En el anterior ejemplo hemos construido una pequeña convnet para resolver el problema de clasificar números escritos a mano usando el dataset MNIST, así que ya estamos familiarizados con la terminología que utiliza keras. Vamos a reutilizar la estructura general que teniamos en el ejemplo anterior: nuestra convnet tendra una pila de capas alternadas de `Conv2D` (con activación `relu` ) y capas  `MaxPooling2D`.\n",
    "\n",
    "Sin embargo, como estamos tratando con imágenes mayores y un problema más complejo, vamos a crear nuestra red en consecuencia: tendrá una capa más de `Conv2D` + `MaxPooling2D`. Esto sirve para aumentar la capacidad de la red y para reducir aún más el tamaño de los mapas de características, para que no sean tan enormes cuando lleguen al paso de aplanado. Empezamos usando imágenes de input de 150x150 (una elección arbitraria), y acabaremos con mapas de características que tienen un tamaño de 7x7 antes de la capa de aplanamiento.\n",
    "\n",
    "Es importante tener en cuenta que la profundidad de los mapas de características va creciendo progresivamente según se avanza en la red neuronales ( de 32 a 128) mientras que el tamaño de los mapas de características va disminuyendo (de 148x148 a 7x7). Este patrón lo verás en casi todas las convnets.\n",
    "\n",
    "Como estamos atacando un problema de clasificación binaria (perro o gato), vamos a acabar la red con una única unidad (una capa densa de tamaño 1) y con una activación sigmoide. Esta unidad codificará la probabilidad de que nuestra red esté mirando a una clase o a otra.\n",
    "\n",
    "El aspecto final del modelo debe de ser el siguiente:\n",
    "\n",
    "![modelo_red_animales.png](https://github.com/laramaktub/MachineLearningI/blob/master/modelo_red_animales.png?raw=true)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32,(3,3) , activation='relu', input_shape=(150,150,3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(64,(3,3) , activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128,(3,3) , activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Conv2D(128,(3,3) , activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para el paso de compilación utilizaremos el optimizador `RMSprop`(lr=1e-4). Como nuestra red termina con una única unidad sigmoide, vamos a utilizar binary crossentropy como nuestra función de pérdida."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "opt = keras.optimizers.rmsprop(lr=0.0001)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Preprocesado de datos\n",
    "\n",
    "Las imágenes deben estar formateadas apropiadamente en tensores de flotantes antes de dárselas a la red. Esto es justo lo que vamos a hacer aqui. Antes de preprocesarlas las imágenes son archivos JPEG. Los pasos para poder darlos a nuestra red son a grandes rasgos:\n",
    "\n",
    "* Leer los archivos con las imágenes.\n",
    "* Decodificar el contenido del JPEG en una \"parrilla\" con el RGB de los pixels \n",
    "* Convertir esa \"parrilla\" en tensores de flotantes\n",
    "* Re-escalar los valores de los pixels (enre 0 y 255) al intervalo [0, 1] ya que las redes neuronales prefieren trabajar con valores pequeños. \n",
    "\n",
    "Todo esto puede parecer muy complicado pero gracias a Keras nuestra vida es mucho más fácil y podemos contar con tus herramientas para ocuparse de estos pasos automaticamente. Keras tiene un módulo con herramientas para el tratamiento de imágenes, que se puede encontrar en  `keras.preprocessing.image`. En particular, contiene la clase `ImageDataGenerator` que nos permite automaticamente convertir imágenes que tengamos en el disco duro en tensores pre-procesados. Esto es justamente lo que usaremos a continuación. Para ello podemos utilizar el flow_from_directory para coger las imágenes directamente de las carpetas que generamos previamente. Le damos como entrada las carpetas donde están las imágenes de entrenamiento (o validación), el tamaño de las imágenes (target_size), tamaño del batch que vamos a usar (vamos a empezar por 20) y como solo hay dos categorías, le decimos que vamos a usar binary_crossentropy (class_mode). Al correr estos comandos obtendremos lo siguiente el número total de imágenes y cuantas clases hay."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carlos\n"
     ]
    }
   ],
   "source": [
    "cd "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1198 images belonging to 2 classes.\n",
      "Found 183 images belonging to 2 classes.\n"
     ]
    }
   ],
   "source": [
    "#ImageDataGenerator: genera dataset de test, train y validation\n",
    "from keras.preprocessing.image import ImageDataGenerator\n",
    "\n",
    "train_datagen = ImageDataGenerator(rescale=1./255)\n",
    "validation_datagen = ImageDataGenerator(rescale=1./255)\n",
    "\n",
    "train_generator = train_datagen.flow_from_directory(directory='Documents/master/Machine Learning I/T03/ImageClassification/gusanos_and_insectos_small/train',target_size=(150,150), class_mode=\"binary\", batch_size=16)\n",
    "validation_generator = validation_datagen.flow_from_directory(directory='Documents/master/Machine Learning I/T03/ImageClassification/gusanos_and_insectos_small/validation',target_size=(150,150), class_mode=\"binary\", batch_size=16)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a echar un vistazo a uno de estos generadores: nos lleva a un batch de 150x150 imágenes RGB (dimensiones `(20, 150, 150, 3)`) y etiquetas binarias (dimensión `(20,)`). 20 es el número de ejemplos en cada batch (lo que llamamos el tamaño del batch). El generador genera estos batches de manera indefinida: corre un bucle sin cesar por todas las imágenes que tengamos en la carpeta. Por eso tenemos que escribir `break` para romper el bucle en algún momento.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "data batch shape: (16, 150, 150, 3)\n",
      "labels batch shape: (16,)\n"
     ]
    }
   ],
   "source": [
    "for data_batch, labels_batch in train_generator:\n",
    "    print('data batch shape:', data_batch.shape)\n",
    "    print('labels batch shape:', labels_batch.shape)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a hacer el fit. En este caso, como lo que tenemos es un generator, utilizamos fit_generator. Vamos a correr 30 épocas y a utilizar el dataset de validación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carlos\\Miniconda3\\envs\\datascience\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n",
      "C:\\Users\\Carlos\\Miniconda3\\envs\\datascience\\lib\\site-packages\\ipykernel_launcher.py:3: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=<keras_pre..., callbacks=[<keras.ca..., steps_per_epoch=62, epochs=30, validation_steps=100)`\n",
      "  This is separate from the ipykernel package so we can avoid doing imports until\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "62/62 [==============================] - 76s 1s/step - loss: 0.3841 - acc: 0.8381 - val_loss: 0.7350 - val_acc: 0.6080\n",
      "Epoch 2/30\n",
      "62/62 [==============================] - 76s 1s/step - loss: 0.3731 - acc: 0.8351 - val_loss: 0.6359 - val_acc: 0.6649\n",
      "Epoch 3/30\n",
      "62/62 [==============================] - 76s 1s/step - loss: 0.3406 - acc: 0.8635 - val_loss: 0.7131 - val_acc: 0.6340\n",
      "Epoch 4/30\n",
      "62/62 [==============================] - 76s 1s/step - loss: 0.3236 - acc: 0.8589 - val_loss: 0.7237 - val_acc: 0.6185\n",
      "Epoch 5/30\n",
      "62/62 [==============================] - 77s 1s/step - loss: 0.2991 - acc: 0.8789 - val_loss: 0.6054 - val_acc: 0.6826\n",
      "Epoch 6/30\n",
      "62/62 [==============================] - 77s 1s/step - loss: 0.3074 - acc: 0.8717 - val_loss: 0.6796 - val_acc: 0.6485\n",
      "Epoch 7/30\n",
      "62/62 [==============================] - 76s 1s/step - loss: 0.2372 - acc: 0.9162 - val_loss: 0.7215 - val_acc: 0.6774\n",
      "Epoch 8/30\n",
      "62/62 [==============================] - 76s 1s/step - loss: 0.2335 - acc: 0.9117 - val_loss: 0.7573 - val_acc: 0.6198\n",
      "Epoch 9/30\n",
      "62/62 [==============================] - 76s 1s/step - loss: 0.2260 - acc: 0.9163 - val_loss: 0.8314 - val_acc: 0.6037\n",
      "Epoch 10/30\n",
      "62/62 [==============================] - 76s 1s/step - loss: 0.1980 - acc: 0.9254 - val_loss: 0.7305 - val_acc: 0.6649\n"
     ]
    }
   ],
   "source": [
    "callbacks = keras.callbacks.EarlyStopping(patience = 5)\n",
    "history = model.fit_generator(train_generator,samples_per_epoch=1000,nb_epoch=30,validation_data=validation_generator,nb_val_samples=100,\n",
    "                             callbacks=[callbacks])\n",
    "#pintar el accuracy"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "Es una buena idea guardar el modelo después de entrenar:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.models import load_model\n",
    "\n",
    "model.save('Documents/master/Machine Learning I/T03/ImageClassification/my_model.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora evalua el modelo en el dataset de test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 182 images belonging to 2 classes.\n",
      "182/182 [==============================] - 50s 276ms/step\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[1.0822064631512682, 0.5760318607868793]"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = train_datagen.flow_from_directory(directory='Documents/master/Machine Learning I/T03/ImageClassification/gusanos_and_insectos_small/test',target_size=(150,150), class_mode=\"binary\", batch_size=16)\n",
    "\n",
    "\n",
    "model.evaluate_generator(test_generator,steps=182,verbose=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Prueba a optimizar la red. Para ello puedes utilizar las herramientas utilizadas en clase. Puedes probar a optimizar en términos de velocidad y de accuracy. Comenta los resultados."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\Users\\Carlos\\Miniconda3\\envs\\datascience\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:3445: calling dropout (from tensorflow.python.ops.nn_ops) with keep_prob is deprecated and will be removed in a future version.\n",
      "Instructions for updating:\n",
      "Please use `rate` instead of `keep_prob`. Rate should be set to `rate = 1 - keep_prob`.\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "conv2d_1 (Conv2D)            (None, 148, 148, 32)      896       \n",
      "_________________________________________________________________\n",
      "max_pooling2d_1 (MaxPooling2 (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "dropout_1 (Dropout)          (None, 74, 74, 32)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_2 (Conv2D)            (None, 72, 72, 64)        18496     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_2 (MaxPooling2 (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "dropout_2 (Dropout)          (None, 36, 36, 64)        0         \n",
      "_________________________________________________________________\n",
      "conv2d_3 (Conv2D)            (None, 34, 34, 128)       73856     \n",
      "_________________________________________________________________\n",
      "max_pooling2d_3 (MaxPooling2 (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "dropout_3 (Dropout)          (None, 17, 17, 128)       0         \n",
      "_________________________________________________________________\n",
      "conv2d_4 (Conv2D)            (None, 15, 15, 128)       147584    \n",
      "_________________________________________________________________\n",
      "max_pooling2d_4 (MaxPooling2 (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "dropout_4 (Dropout)          (None, 7, 7, 128)         0         \n",
      "_________________________________________________________________\n",
      "flatten_1 (Flatten)          (None, 6272)              0         \n",
      "_________________________________________________________________\n",
      "dense_1 (Dense)              (None, 512)               3211776   \n",
      "_________________________________________________________________\n",
      "dense_2 (Dense)              (None, 1)                 513       \n",
      "=================================================================\n",
      "Total params: 3,453,121\n",
      "Trainable params: 3,453,121\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from keras import layers\n",
    "from keras import models\n",
    "from keras import backend as K\n",
    "\n",
    "K.clear_session()\n",
    "\n",
    "model = models.Sequential()\n",
    "model.add(layers.Conv2D(32,(3,3) , activation='relu', input_shape=(150,150,3)))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Conv2D(64,(3,3) , activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Conv2D(128,(3,3) , activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Conv2D(128,(3,3) , activation='relu'))\n",
    "model.add(layers.MaxPooling2D((2,2)))\n",
    "model.add(layers.Dropout(0.2))\n",
    "model.add(layers.Flatten())\n",
    "model.add(layers.Dense(512, activation='relu'))\n",
    "model.add(layers.Dense(1, activation='sigmoid'))\n",
    "\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Carlos\\Miniconda3\\envs\\datascience\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: The semantics of the Keras 2 argument `steps_per_epoch` is not the same as the Keras 1 argument `samples_per_epoch`. `steps_per_epoch` is the number of batches to draw from the generator at each epoch. Basically steps_per_epoch = samples_per_epoch/batch_size. Similarly `nb_val_samples`->`validation_steps` and `val_samples`->`steps` arguments have changed. Update your method calls accordingly.\n",
      "  # Remove the CWD from sys.path while we load stuff.\n",
      "C:\\Users\\Carlos\\Miniconda3\\envs\\datascience\\lib\\site-packages\\ipykernel_launcher.py:10: UserWarning: Update your `fit_generator` call to the Keras 2 API: `fit_generator(<keras_pre..., validation_data=<keras_pre..., callbacks=[<keras.ca..., steps_per_epoch=62, epochs=30, validation_steps=100)`\n",
      "  # Remove the CWD from sys.path while we load stuff.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/30\n",
      "62/62 [==============================] - 85s 1s/step - loss: 0.6928 - acc: 0.5060 - val_loss: 0.6924 - val_acc: 0.5085\n",
      "Epoch 2/30\n",
      "62/62 [==============================] - 86s 1s/step - loss: 0.6918 - acc: 0.5009 - val_loss: 0.6896 - val_acc: 0.5155\n",
      "Epoch 3/30\n",
      "62/62 [==============================] - 90s 1s/step - loss: 0.6861 - acc: 0.5554 - val_loss: 0.6837 - val_acc: 0.5164\n",
      "Epoch 4/30\n",
      "62/62 [==============================] - 70s 1s/step - loss: 0.6699 - acc: 0.5963 - val_loss: 0.6638 - val_acc: 0.5707\n",
      "Epoch 5/30\n",
      "62/62 [==============================] - 56s 907ms/step - loss: 0.6702 - acc: 0.5948 - val_loss: 0.6517 - val_acc: 0.6169\n",
      "Epoch 6/30\n",
      "62/62 [==============================] - 57s 915ms/step - loss: 0.6620 - acc: 0.6038 - val_loss: 0.6607 - val_acc: 0.5884\n",
      "Epoch 7/30\n",
      "62/62 [==============================] - 64s 1s/step - loss: 0.6519 - acc: 0.6190 - val_loss: 0.6349 - val_acc: 0.6891\n",
      "Epoch 8/30\n",
      "62/62 [==============================] - 64s 1s/step - loss: 0.6594 - acc: 0.6210 - val_loss: 0.6363 - val_acc: 0.6722\n",
      "Epoch 9/30\n",
      "62/62 [==============================] - 61s 980ms/step - loss: 0.6329 - acc: 0.6442 - val_loss: 0.6437 - val_acc: 0.6780\n",
      "Epoch 10/30\n",
      "62/62 [==============================] - 61s 992ms/step - loss: 0.6387 - acc: 0.6518 - val_loss: 0.6416 - val_acc: 0.6662\n",
      "Epoch 11/30\n",
      "62/62 [==============================] - 64s 1s/step - loss: 0.6472 - acc: 0.6254 - val_loss: 0.6452 - val_acc: 0.6524\n",
      "Epoch 12/30\n",
      "62/62 [==============================] - 65s 1s/step - loss: 0.6230 - acc: 0.6449 - val_loss: 0.6327 - val_acc: 0.6675\n",
      "Epoch 13/30\n",
      "62/62 [==============================] - 61s 983ms/step - loss: 0.6204 - acc: 0.6704 - val_loss: 0.6249 - val_acc: 0.6682\n",
      "Epoch 14/30\n",
      "62/62 [==============================] - 59s 947ms/step - loss: 0.6215 - acc: 0.6503 - val_loss: 0.6297 - val_acc: 0.6649\n",
      "Epoch 15/30\n",
      "62/62 [==============================] - 59s 951ms/step - loss: 0.6084 - acc: 0.6650 - val_loss: 0.6241 - val_acc: 0.6793\n",
      "Epoch 16/30\n",
      "62/62 [==============================] - 76s 1s/step - loss: 0.5902 - acc: 0.7003 - val_loss: 0.6092 - val_acc: 0.6839\n",
      "Epoch 17/30\n",
      "62/62 [==============================] - 82s 1s/step - loss: 0.5863 - acc: 0.6740 - val_loss: 0.6189 - val_acc: 0.6728\n",
      "Epoch 18/30\n",
      "62/62 [==============================] - 82s 1s/step - loss: 0.5803 - acc: 0.6806 - val_loss: 0.6114 - val_acc: 0.6839\n",
      "Epoch 19/30\n",
      "62/62 [==============================] - 85s 1s/step - loss: 0.5553 - acc: 0.7157 - val_loss: 0.6095 - val_acc: 0.6538\n",
      "Epoch 20/30\n",
      "62/62 [==============================] - 82s 1s/step - loss: 0.5659 - acc: 0.7062 - val_loss: 0.6229 - val_acc: 0.6893\n",
      "Epoch 21/30\n",
      "62/62 [==============================] - 84s 1s/step - loss: 0.5477 - acc: 0.7143 - val_loss: 0.6219 - val_acc: 0.6662\n"
     ]
    }
   ],
   "source": [
    "from keras import optimizers\n",
    "\n",
    "opt = keras.optimizers.Adam(lr=0.0001)\n",
    "callbacks = keras.callbacks.EarlyStopping(patience = 5)\n",
    "model.compile(optimizer=opt,\n",
    "              loss='binary_crossentropy',\n",
    "              metrics=['accuracy'])\n",
    "\n",
    "history = model.fit_generator(train_generator,samples_per_epoch=1000,nb_epoch=30,validation_data=validation_generator,nb_val_samples=100,\n",
    "                             callbacks=[callbacks])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('Documents/master/Machine Learning I/T03/ImageClassification/my_model.h5')  # creates a HDF5 file 'my_model.h5'"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
